{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start for Beginners\n",
    "\n",
    "<!-- TOC -->\n",
    "\n",
    "- [Quick Start for Beginners](#quick-start-for-beginners)\n",
    "    - [Configuring the Running Information](#configuring-the-running-information)\n",
    "    - [Downloading the Dataset](#downloading-the-dataset)\n",
    "    - [Data Processing](#data-processing)\n",
    "    - [Creating a Model](#creating-a-model)\n",
    "    - [Optimizing Model Parameters](#optimizing-model-parameters)\n",
    "    - [Training and Saving the Model](#training-and-saving-the-model)\n",
    "    - [Loading the Model](#loading-the-model)\n",
    "    - [Validating the Model](#validating-the-model)\n",
    "\n",
    "<!-- /TOC -->\n",
    "\n",
    "## Configuring the Running Information\n",
    "\n",
    "MindSpore uses `context.set_context` to configure the information required for running, such as the running mode, backend information, and hardware information.\n",
    "\n",
    "Import the `context` module and configure the required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from mindspore import context\n",
    "\n",
    "parser = argparse.ArgumentParser(description='MindSpore LeNet Example')\n",
    "parser.add_argument('--device_target', type=str, default=\"Ascend\", choices=['Ascend', 'GPU', 'CPU'])\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example runs in graph mode. You can configure hardware information as required. For example, if the code runs on the Ascend AI processor, set `--device_target` to `Ascend`. This rule also applies to the code running on the CPU and GPU. For details about the parameters, see [context.set_context](https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.context.html).\n",
    "\n",
    "## Downloading the Dataset\n",
    "\n",
    "The MNIST dataset used in this example consists of 10 classes of 28 x 28 pixels grayscale images. It has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
    "\n",
    "Click [here](http://yann.lecun.com/exdb/mnist/) to download the MNIST dataset and place the dataset according to the following directory structure. \n",
    "\n",
    "```text\n",
    "    ./datasets/MNIST_Data\n",
    "    ├── test\n",
    "    │   ├── t10k-images-idx3-ubyte\n",
    "    │   └── t10k-labels-idx1-ubyte\n",
    "    └── train\n",
    "        ├── train-images-idx3-ubyte\n",
    "        └── train-labels-idx1-ubyte\n",
    "\n",
    "    2 directories, 4 files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Datasets are crucial for model training. A good dataset can effectively improve training accuracy and efficiency.\n",
    "MindSpore provides the API module `mindspore.dataset` for data processing to store samples and labels. Before loading a dataset, we usually process the dataset. `mindspore.dataset` integrates common data processing methods.\n",
    "\n",
    "Import `mindspore.dataset` and other corresponding modules in MindSpore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore import dtype as mstype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset processing consists of the following steps:\n",
    "\n",
    "1. Define the `create_dataset` function to create a dataset.\n",
    "2. Define the data augmentation and processing operations to prepare for subsequent mapping.\n",
    "3. Use the map function to apply data operations to the dataset.\n",
    "4. Perform shuffle and batch operations on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    # Define the dataset.\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # Define the mapping to be operated.\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    hwc2chw_op = CV.HWC2CHW()\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "\n",
    "    # Use the map function to apply data operations to the dataset.\n",
    "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=resize_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_nml_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=hwc2chw_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    # Perform shuffle and batch operations.\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return mnist_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding information, `batch_size` indicates the number of data records in each group. Assume that each group contains 32 data records.\n",
    "\n",
    "> MindSpore supports multiple data processing and argumentation operations. For details, see [Processing Data](https://www.mindspore.cn/doc/programming_guide/en/r1.2/pipeline.html) and [Data Augmentation](https://www.mindspore.cn/doc/programming_guide/en/r1.2/augmentation.html).\n",
    "\n",
    "## Creating a Model\n",
    "\n",
    "To use MindSpore for neural network definition, inherit `mindspore.nn.Cell`. `Cell` is the base class of all neural networks (such as `Conv2d-relu-softmax`).\n",
    "\n",
    "Define each layer of a neural network in the `__init__` method in advance, and then define the `construct` method to complete the forward construction of the neural network. According to the LeNet structure, define the network layers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import Normal\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"\n",
    "    Lenet network structure\n",
    "    \"\"\"\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # Define the required operation.\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # Use the defined operation to construct a forward network.\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network.\n",
    "net = LeNet5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more information about building a neural network in MindSpore, see [Defining the Network](https://www.mindspore.cn/tutorial/training/en/r1.2/use/defining_the_network.html).\n",
    "\n",
    "## Optimizing Model Parameters\n",
    "\n",
    "To train a neural network model, a loss function and an optimizer need to be defined.\n",
    "\n",
    "Loss functions supported by MindSpore include `SoftmaxCrossEntropyWithLogits`, `L1Loss`, and `MSELoss`. The following uses the cross-entropy loss function `SoftmaxCrossEntropyWithLogits`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function.\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more information about using loss functions in MindSpore, see [Loss Functions](https://www.mindspore.cn/tutorial/en/r1.2/optimization.html#loss-functions).\n",
    "\n",
    "MindSpore supports the `Adam`, `AdamWeightDecay`, and `Momentum` optimizers. The following uses the `Momentum` optimizer as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer.\n",
    "net_opt = nn.Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more information about using an optimizer in MindSpore, see [Optimizer](https://www.mindspore.cn/tutorial/en/r1.2/optimization.html#optimizer).\n",
    "\n",
    "## Training and Saving the Model\n",
    "\n",
    "MindSpore provides the callback mechanism to execute custom logic during training. The following uses `ModelCheckpoint` provided by the framework as an example.\n",
    "`ModelCheckpoint` can save the network model and parameters for subsequent fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
    "# Set model saving parameters.\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "# Use model saving parameters.\n",
    "ckpoint = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.train` API provided by MindSpore can be used to easily train the network. `LossMonitor` can monitor the changes of the `loss` value during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library required for model training.\n",
    "from mindspore.nn import Accuracy\n",
    "from mindspore.train.callback import LossMonitor\n",
    "from mindspore import Model\n",
    "\n",
    "def train_net(args, model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):\n",
    "    \"\"\"Define a training method.\"\"\"\n",
    "    # Load the training dataset.\n",
    "    ds_train = create_dataset(os.path.join(data_path, \"train\"), 32, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(125)], dataset_sink_mode=sink_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset_sink_mode` is used to control whether data is offloaded. Data offloading means that data is directly transmitted to the device through a channel to accelerate the training speed. If `dataset_sink_mode` is True, data is offloaded. Otherwise, data is not offloaded.\n",
    "\n",
    "Validate the generalization capability of the model based on the result obtained by running the test dataset.\n",
    "\n",
    "1. Read the test dataset using the `model.eval` API.\n",
    "2. Use the saved model parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(network, model, data_path):\n",
    "    \"\"\"Define a validation method.\"\"\"\n",
    "    ds_eval = create_dataset(os.path.join(data_path, \"test\"))\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
    "    print(\"{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `train_epoch` to 1 to train the dataset in one epoch. In the `train_net` and `test_net` methods, the previously downloaded training dataset is loaded. `mnist_path` is the path of the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 125, loss is 0.077120684\n",
      "epoch: 1 step: 250, loss is 0.052946586\n",
      "epoch: 1 step: 375, loss is 0.11754811\n",
      "epoch: 1 step: 500, loss is 0.013226306\n",
      "epoch: 1 step: 625, loss is 0.13013957\n",
      "epoch: 1 step: 750, loss is 0.031838104\n",
      "epoch: 1 step: 875, loss is 0.0073729972\n",
      "epoch: 1 step: 1000, loss is 0.31429246\n",
      "epoch: 1 step: 1125, loss is 0.15579891\n",
      "epoch: 1 step: 1250, loss is 0.07803872\n",
      "epoch: 1 step: 1375, loss is 0.011987055\n",
      "epoch: 1 step: 1500, loss is 0.044265144\n",
      "epoch: 1 step: 1625, loss is 0.038708974\n",
      "epoch: 1 step: 1750, loss is 0.065863356\n",
      "epoch: 1 step: 1875, loss is 0.15959188\n",
      "{'Accuracy': 0.9765625}\n"
     ]
    }
   ],
   "source": [
    "train_epoch = 1\n",
    "mnist_path = \"./datasets/MNIST_Data\"\n",
    "dataset_size = 1\n",
    "model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "train_net(args, model, train_epoch, mnist_path, dataset_size, ckpoint, False)\n",
    "test_net(net, model, mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to execute the script:\n",
    "\n",
    "```bash\n",
    "python lenet.py --device_target=Ascend\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "`lenet.py`: You can paste the preceding code to lenet.py (excluding the code for downloading the dataset). Generally, you can move the import part to the beginning of the code, place the definitions of classes, functions, and methods after the code, and connect the preceding operations in the main method.\n",
    "\n",
    "`--device_target=CPU`: specifies the running hardware platform. The parameter value can be `CPU`, `GPU`, or `Ascend`, depending on the actual running hardware platform.\n",
    "\n",
    "Loss values are displayed during training, as shown in the following. Although loss values may fluctuate, they gradually decrease and the accuracy gradually increases in general. Loss values displayed each time may be different because of their randomicity.\n",
    "The following is an example of loss values output during training:\n",
    "\n",
    "```text\n",
    "epoch: 1 step: 125, loss is 2.3083377\n",
    "epoch: 1 step: 250, loss is 2.3019726\n",
    "...\n",
    "epoch: 1 step: 1500, loss is 0.028385757\n",
    "epoch: 1 step: 1625, loss is 0.0857362\n",
    "epoch: 1 step: 1750, loss is 0.05639569\n",
    "epoch: 1 step: 1875, loss is 0.12366105\n",
    "{'Accuracy': 0.9663477564102564}\n",
    "```\n",
    "\n",
    "The model accuracy data is displayed in the output content. In the example, the accuracy reaches 96.6%, indicating a good model quality. As the number of network epochs (`train_epoch`) increases, the model accuracy will be further improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "# Load the saved model for testing.\n",
    "param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
    "# Load parameters to the network.\n",
    "load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more information about loading a model in MindSpore, see [Loading the Model](https://www.mindspore.cn/tutorial/en/r1.2/save_load_model.html#loading-the-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Model\n",
    "\n",
    "Use the generated model to predict the classification of a single image. The procedure is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"1\", Actual: \"1\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "\n",
    "# Define a test dataset. If batch_size is set to 1, an image is obtained.\n",
    "ds_test = create_dataset(os.path.join(mnist_path, \"test\"), batch_size=1).create_dict_iterator()\n",
    "data = next(ds_test)\n",
    "\n",
    "# `images` indicates the test image, and `labels` indicates the actual classification of the test image.\n",
    "images = data[\"image\"].asnumpy()\n",
    "labels = data[\"label\"].asnumpy()\n",
    "\n",
    "# Use the model.predict function to predict the classification of the image.\n",
    "output = model.predict(Tensor(data['image']))\n",
    "predicted = np.argmax(output.asnumpy(), axis=1)\n",
    "\n",
    "# Output the predicted classification and the actual classification.\n",
    "print(f'Predicted: \"{predicted[0]}\", Actual: \"{labels[0]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
